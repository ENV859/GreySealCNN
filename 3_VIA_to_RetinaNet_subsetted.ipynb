{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "3_VIA_to_RetinaNet_subsetted.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWws_Zmzl2U0"
      },
      "source": [
        "# Subset and convert VIA annotations CSV file to RetinaNet CSV format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adbbeiaql2VD"
      },
      "source": [
        "#### Before running this script, make sure that your Google Drive folder contains all of the tiles and the `spatial_data.json` that you created (step 1), and the annotations `csv` that you exported from VIA (step 2)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GgYkvC1l2VD"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gl7176/GreySealCNN/blob/master/3_VIA_to_RetinaNet_subsetted.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "#####  <center> Be sure to update this hyperlink above if you clone and want to point to a different GitHub </center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L18dIkUTl2VD"
      },
      "source": [
        "### Connect to our Google Drive folder and pull annotation CSV\n",
        "Note: when you run this it will give you a link that you must click. You must give Google some permissions, then copy a code into a box that comes up in the output section of this code.\n",
        "\n",
        "If customizing this code, you will need to point the `drive_folder` variable to a URL for your shared google drive folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtI8Tc28l2VE",
        "outputId": "d40b5747-af5f-4355-ba5d-ccd86eeee007"
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "import os, json, csv\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# choose a local (colab) directory to store the data.\n",
        "local_download_path = os.path.expanduser('VIA_annotations')\n",
        "try:\n",
        "  os.makedirs(local_download_path)\n",
        "except: pass\n",
        "\n",
        "# 2. Auto-iterate using the query syntax\n",
        "#    https://developers.google.com/drive/v2/web/search-parameters\n",
        "\n",
        "# set variable to the destination google drive folder you want to pull from\n",
        "drive_folder = 'https://drive.google.com/drive/folders/1INuRNVKvKMy8L_Nb6lmoVbyvScWK0-0D'\n",
        "\n",
        "# this bit points the code to that google drive folder\n",
        "pointer = str(\"'\" + drive_folder.split(\"/\")[-1] + \"'\" + \" in parents\")\n",
        "\n",
        "file_list = drive.ListFile(\n",
        "    {'q': pointer}).GetList()\n",
        "\n",
        "# this bit pulls the necessary files from the directory specified above\n",
        "# and compiles a list of all png images (tiles) in the same directory\n",
        "\n",
        "\n",
        "annotations_file = {}\n",
        "image_list = []\n",
        "\n",
        "count = 0\n",
        "for f in file_list:\n",
        "  count += 1\n",
        "  if count % 10 == 0:\n",
        "    print(count)\n",
        "  # 3. Create & download CSV annotations file\n",
        "  fname = os.path.join(local_download_path, f['title'])\n",
        "  # detect spatial data file based on first dictionary entry\n",
        "  if fname.endswith(\".json\"):\n",
        "    spatial_data_candidate = fname\n",
        "    f_ = drive.CreateFile({'id': f['id']})\n",
        "    f_.GetContentFile(spatial_data_candidate)\n",
        "    with open(spatial_data_candidate) as f:\n",
        "      try:\n",
        "        json.load(f)[\"tile_heght\"]\n",
        "        spatial_data_file = spatial_data_candidate\n",
        "      except: continue\n",
        "  # list png files\n",
        "  if fname.endswith(\".png\"):\n",
        "    image_list.append(fname.split(\"/\")[1])\n",
        "  # detect annotations file based on first three column names\n",
        "  if fname.endswith(\".csv\"): \n",
        "      annotations_candidate = fname\n",
        "      f_ = drive.CreateFile({'id': f['id']})\n",
        "      f_.GetContentFile(annotations_candidate)\n",
        "      with open(annotations_candidate, \"r\") as f:\n",
        "       if next(csv.reader(f, delimiter=\",\"))[0:3] == ['filename', 'file_size', 'file_attributes']:\n",
        "         annotations_file = annotations_candidate\n",
        "       else: continue\n",
        "print(\"annotations file identified as \" + annotations_file)\n",
        "print(\"tile data file identified as \" + spatial_data_file)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n",
            "20\n",
            "30\n",
            "40\n",
            "50\n",
            "60\n",
            "70\n",
            "80\n",
            "90\n",
            "100\n",
            "110\n",
            "120\n",
            "130\n",
            "140\n",
            "150\n",
            "160\n",
            "170\n",
            "180\n",
            "190\n",
            "200\n",
            "210\n",
            "220\n",
            "230\n",
            "240\n",
            "250\n",
            "260\n",
            "270\n",
            "280\n",
            "annotations file identified as VIA_annotations/via_SealCNN_TrainingData.csv\n",
            "tile data file identified as VIA_annotations/spatial_data.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBL3WNA6l2VE"
      },
      "source": [
        "### Set up the python environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIIq8rFPl2VF"
      },
      "source": [
        "# import necessary modules\n",
        "import os, csv, random, json\n",
        "\n",
        "# if running code on a local machine, manually point to the annotations_file here\n",
        "annotations_file = annotations_file\n",
        "\n",
        "# set pseudo-random values for replicability\n",
        "random.seed(1)\n",
        "\n",
        "# use this variable to set output directory\n",
        "output_dir = 'RetinaNet_annotations'\n",
        "\n",
        "# create the directory if it doesn't already exist\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-adKpUfvW9B"
      },
      "source": [
        "### Confirm that we have all tiles accounted for\r\n",
        "by comparing our json from step 1 with our file list pulled from the googe drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKnxXFiMmaqL"
      },
      "source": [
        "# open the output from our original tile splitting\r\n",
        "with open(spatial_data_file) as f:\r\n",
        "    tile_list = json.load(f)[\"tile_pointers\"][\"image_locations\"]\r\n",
        "\r\n",
        "if len(tile_list) != len(image_list):\r\n",
        "  print('Step one produced {n1} tiles, but google drive contained {n2} images. Confirm that tile set is complete.\\n'.format(n1=len(tile_list), n2=len(image_list)))\r\n",
        "  raise Exception(\"tile count mismatch\")"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eheLKPfBl2VF"
      },
      "source": [
        "### Shuffle and split images into 3 datasets: Training, Testing, Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jt_McLbfl2VF",
        "outputId": "bde1a3dd-55a9-414a-9baf-c9dd8dad8386"
      },
      "source": [
        "# shuffle the image list randomly and get total count\n",
        "random.shuffle(image_list)\n",
        "total_count = len(image_list)\n",
        "\n",
        "# set indices for breaking up the total dataset into TTV parts\n",
        "test_fraction, valid_fraction, train_fraction = 0.1, 0.04, 0.86\n",
        "\n",
        "# spit error if the math don't add up\n",
        "if (sum([test_fraction, valid_fraction, train_fraction]) != 1.0):\n",
        "   raise Exception(\"fractions should add up to 1\")\n",
        "\n",
        "test_index = int(total_count * test_fraction)\n",
        "valid_index = int(total_count * (test_fraction + valid_fraction))\n",
        "\n",
        "# use indices to break up dataset into the three parts\n",
        "test_dataset, valid_dataset, train_dataset = image_list[:test_index], image_list[test_index:valid_index], image_list[valid_index:]\n",
        "print(len(test_dataset), len(valid_dataset), len(train_dataset))\n",
        "\n",
        "# spit out CSV listing the image subsets\n",
        "subset_list = []\n",
        "for row in test_dataset:\n",
        "        subset_list.append([row, \"testing\"])\n",
        "for row in valid_dataset:\n",
        "        subset_list.append([row, \"validation\"])\n",
        "for row in train_dataset:\n",
        "        subset_list.append([row, \"training\"])\n",
        "with open(output_dir + '/subset_list.csv', 'w', newline='') as fp:\n",
        "    writer = csv.writer(fp)\n",
        "    writer.writerows(subset_list)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "27 11 234\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoMTsew-l2VF"
      },
      "source": [
        "### Reformat annotations from VIA to RetinaNet format\n",
        "The following loop pulls each annotation, line-by-line, from the VIA exported CSV, extracts the necessary information, reformats it into the format that RetinaNet requires (https://github.com/fizyr/keras-retinanet#annotations-format), then reassembles a new CSV line-by-line that RetinaNet can receive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "DQ2btGJRl2VG"
      },
      "source": [
        "# Create blank variable for each annotations list as we build it\n",
        "image_annotations_train, image_annotations_test, image_annotations_valid = [], [], []\n",
        "\n",
        "# Create blank list for class names\n",
        "class_list = []\n",
        "\n",
        "# read each line, parse it, convert it, put it all back together\n",
        "# then drop it in the appropriate subset\n",
        "with open(annotations_file, \"r\") as f:\n",
        "    reader = csv.reader(f, delimiter=\",\")\n",
        "    for line in reader: \n",
        "        # output we want:\n",
        "        # format: path/to/image.jpg,x1,y1,x2,y2,class_name\n",
        "        # example: /data/imgs/img_001.jpg,837,346,981,456,cow\n",
        "        if 'filename' in line[0]:\n",
        "            # bypassing comments in csv\n",
        "            continue\n",
        "        if '{}' in line[5]:\n",
        "            #bypassing empty images\n",
        "            continue\n",
        "            \n",
        "        filename = line[0]\n",
        "        \n",
        "        # pulling from column named \"region_shape_attributes\"\n",
        "        box_entry = json.loads(line[5])\n",
        "        top_left_x, top_left_y, width, height = box_entry[\"x\"], box_entry[\"y\"], box_entry[\"width\"], box_entry[\"height\"]\n",
        " \n",
        "        if width == 0 or height == 0:\n",
        "            continue\n",
        "            # skip tiny/empty boxes\n",
        "        \n",
        "        # convert from \"top left and width/height\" to \"x and y values at each corner of the box\"\n",
        "        if top_left_x < 0:\n",
        "            top_left_x = 1\n",
        "        if top_left_y < 0:\n",
        "            top_left_y = 1\n",
        "        x1, x2, y1, y2 = top_left_x, top_left_x + width, top_left_y, top_left_y + height \n",
        "        \n",
        "        # pulling from column named \"region_attributes\" to get class names\n",
        "        name = json.loads(line[6])[\"Age Class\"]\n",
        "\n",
        "        # skip unknown class, in this case. Might be useful in other applications though,\n",
        "        # e.g. total object count irrespective of class\n",
        "        if name == \"Unknown\":\n",
        "            continue\n",
        "\n",
        "        # build list of classes as we encounter new names\n",
        "        if name not in class_list:\n",
        "            class_list.append(name)\n",
        "\n",
        "          # create the annotation row\n",
        "        new_row = [filename, x1, y1, x2, y2, name]\n",
        "        \n",
        "        # append the row to the correct subset (training, testing, or validation)\n",
        "        if filename in train_dataset:\n",
        "            image_annotations_train.append(new_row)\n",
        "        elif filename in test_dataset:\n",
        "            image_annotations_test.append(new_row)\n",
        "        else:\n",
        "            image_annotations_valid.append(new_row)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NwUNk5Pl2VH"
      },
      "source": [
        "### Output annotations.csv and classes.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6K_ihMuHl2VH"
      },
      "source": [
        "with open(output_dir + '/annotations_train.csv', 'w', newline='') as fp:\n",
        "    writer = csv.writer(fp)\n",
        "    writer.writerows(image_annotations_train)\n",
        "\n",
        "with open(output_dir + '/annotations_test.csv', 'w', newline='') as fp:\n",
        "    writer = csv.writer(fp)\n",
        "    writer.writerows(image_annotations_test)\n",
        "\n",
        "with open(output_dir + '/annotations_valid.csv', 'w', newline='') as fp:\n",
        "    writer = csv.writer(fp)\n",
        "    writer.writerows(image_annotations_valid)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sz1yu_PHl2VI"
      },
      "source": [
        "# this bit uses our class_list (built during annotations processing) to create our classes file\n",
        "# note again that \"unknown\" ambiguous cases have been excluded in this case\n",
        "\n",
        "detection_classes = []\n",
        "\n",
        "for i in range(0, len(class_list)):\n",
        "    detection_classes.append([class_list[i], i])\n",
        "\n",
        "with open(output_dir + '/classes.csv', 'w', newline='') as fp:\n",
        "    writer = csv.writer(fp)\n",
        "    writer.writerows(detection_classes)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKr-mRmkl2VI"
      },
      "source": [
        "#### Zip data folder for download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "hJ-GFG2ql2VI",
        "outputId": "8a3513ec-bc4b-4644-b1a3-a7dd974c65ae"
      },
      "source": [
        "# zip up the output directory into an archive for download\n",
        "import subprocess\n",
        "output_file_name = 'Step_3_{o}'.format(o=output_dir)\n",
        "subprocess.call(['zip', '-r', output_file_name + '.zip', '/content/' + output_dir])\n",
        "\n",
        "from google.colab import files\n",
        "files.download(output_file_name + \".zip\")"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_2f164461-4c09-4a7d-99a1-3ff8a907ccb4\", \"Step_3_RetinaNet_annotations.zip\", 30765)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8kyBoYEl2VI"
      },
      "source": [
        "##### At the end of this script you should have downloaded and 5 `csv` files (Testing, Training and Validation annotation subsets, the subset list  and the classes list). Drop these all in the google directory so they can be ingested by our CNN code in the next step.\n",
        "\n",
        "Next steps:\n",
        "\n",
        "4) train, refine, and test CNN using VIA annotations and the tiles generated here\n",
        "\n",
        "5) export CNN outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FH6aoGHxl2VJ"
      },
      "source": [
        "##### Not a code issue, but I recommend manually checking each annotations document to make sure that there are a reasonable number of annotations in each dataset; it is possible that the random breakdown could pick a cluster of \"empty\" images for validation or training. Should not be an issue in this specific case because I've checked it for our random seed, but in future applications this is a good idea."
      ]
    }
  ]
}