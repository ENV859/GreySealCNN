{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "export_detections.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8WTbTOrpVVf"
      },
      "source": [
        "## Auditing and Exporting Detections"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o97tafnarHqG"
      },
      "source": [
        "#### Before running this script, make sure that your Google Drive folder contains the orthomosaic GeoTiff (`step 0`) and no other GeoTIFF files, the `data.json` file (`step 1`), the `classes.csv` file (`step3`) and the `new_detections.json` file (`step 4`). If you want to add your new CNN detections to the manually annotated detections you created in VIA, also add the JSON file you exported using VIA (`step 2`). You will need to input that file name directly, since it is not standardized in our workflow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QI6JfXBBm1rs"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gl7176/GreySealCNN/blob/master/5_export_detections.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "#####  <center> Be sure to update this hyperlink above if you clone and want to point to a different GitHub </center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0SGgVrGEWZN"
      },
      "source": [
        "### Connect to our Google Drive folder and pull annotation CSV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYEIuuoGpkZm",
        "outputId": "89216ad5-09ed-49ec-e712-284dfd818a72"
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# choose a local (colab) directory to store the data.\n",
        "local_download_path = os.path.expanduser('data')\n",
        "try:\n",
        "  os.makedirs(local_download_path)\n",
        "except: pass\n",
        "\n",
        "# 2. Auto-iterate using the query syntax\n",
        "#    https://developers.google.com/drive/v2/web/search-parameters\n",
        "\n",
        "# set variable to the destination google drive folder you want to pull from\n",
        "drive_folder = 'https://drive.google.com/drive/folders/1INuRNVKvKMy8L_Nb6lmoVbyvScWK0-0D'\n",
        "\n",
        "# this bit points the code to that google drive folder\n",
        "pointer = str(\"'\" + drive_folder.split(\"/\")[-1] + \"'\" + \" in parents\")\n",
        "\n",
        "file_list = drive.ListFile(\n",
        "    {'q': pointer}).GetList()\n",
        "\n",
        "#    this bit pulls key files from the directory specified above\n",
        "orthomosaic_file = {}\n",
        "for f in file_list:\n",
        "  # 3. Create & download by id.\n",
        "  fname = os.path.join(local_download_path, f['title'])\n",
        "  if fname.endswith(\".tif\") or fname.endswith(\".json\") or fname.endswith(\"classes.csv\"):\n",
        "    f_ = drive.CreateFile({'id': f['id']})\n",
        "    f_.GetContentFile(fname)\n",
        "    os.stat(fname)\n",
        "    # if the file is a *.tif and larger than 100 mb we label it the orthomosaic\n",
        "    if fname.endswith(\".tif\") and os.stat(fname).st_size > 10^8 :\n",
        "      # if there are multiple orthomosaic files detected we spit an error\n",
        "      if len(orthomosaic_file) != 0:\n",
        "        raise Exception(\"more than one orthomosaic file identified based on size and type\")\n",
        "      orthomosaic_file = fname\n",
        "      print(\"orthomosaic identified as \" + orthomosaic_file)\n",
        "print(\"all files pulled\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "orthomosaic identified as data/Hay Island 2015.tif\n",
            "all files pulled\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHksaZEPEWZO"
      },
      "source": [
        "### Set up the python environment and key variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biwSnxfdpVVj",
        "outputId": "5f9d41d0-a539-4899-dc97-b641abc136c6"
      },
      "source": [
        "import os\n",
        "import argparse\n",
        "import numpy as np\n",
        "import json\n",
        "import csv\n",
        "!pip install rasterio\n",
        "import rasterio\n",
        "\n",
        "!shapely.geometry import mapping, Polygon\n",
        "!pip install fiona\n",
        "import fiona # only required for exporting to shapefiles"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: rasterio in /usr/local/lib/python3.6/dist-packages (1.1.8)\n",
            "Requirement already satisfied: click<8,>=4.0 in /usr/local/lib/python3.6/dist-packages (from rasterio) (7.1.2)\n",
            "Requirement already satisfied: snuggs>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from rasterio) (1.4.7)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.6/dist-packages (from rasterio) (20.2.0)\n",
            "Requirement already satisfied: click-plugins in /usr/local/lib/python3.6/dist-packages (from rasterio) (1.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from rasterio) (1.18.5)\n",
            "Requirement already satisfied: affine in /usr/local/lib/python3.6/dist-packages (from rasterio) (2.3.0)\n",
            "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.6/dist-packages (from rasterio) (0.7.0)\n",
            "Requirement already satisfied: pyparsing>=2.1.6 in /usr/local/lib/python3.6/dist-packages (from snuggs>=1.4.1->rasterio) (2.4.7)\n",
            "/bin/bash: shapely.geometry: command not found\n",
            "Requirement already satisfied: fiona in /usr/local/lib/python3.6/dist-packages (1.8.18)\n",
            "Requirement already satisfied: six>=1.7 in /usr/local/lib/python3.6/dist-packages (from fiona) (1.15.0)\n",
            "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.6/dist-packages (from fiona) (0.7.0)\n",
            "Requirement already satisfied: munch in /usr/local/lib/python3.6/dist-packages (from fiona) (2.5.0)\n",
            "Requirement already satisfied: click<8,>=4.0 in /usr/local/lib/python3.6/dist-packages (from fiona) (7.1.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from fiona) (2020.6.20)\n",
            "Requirement already satisfied: click-plugins>=1.0 in /usr/local/lib/python3.6/dist-packages (from fiona) (1.1.1)\n",
            "Requirement already satisfied: attrs>=17 in /usr/local/lib/python3.6/dist-packages (from fiona) (20.2.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xrjsw-07pVVz"
      },
      "source": [
        "### Exporting to Shapefile"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIisHMg-7CGj",
        "outputId": "4f9e4ebe-e11b-4423-daec-d6755568f374",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# update box locations from local tile coordinates to orthomosaic coordinates\n",
        "# using image_locations from original tiling process, invoked by filename\n",
        "# ---this involves transforming x1, y1, x2, y2 coordinates to 4 sets of x,y points\n",
        "# and then transforming them back to the original form---\n",
        "# then build a list of detection dictionaries. Note that filename is no longer\n",
        "# needed because coordinates are now relative to the orthomosaic, not tile\n",
        "\n",
        "# open the output from our CNN\n",
        "with open(local_download_path + '/new_detections.json') as f:\n",
        "    detected_labels = json.load(f)\n",
        "\n",
        "# open the output from our original tile splitting\n",
        "with open(local_download_path + '/data.json') as f:\n",
        "  img_data = json.load(f)\n",
        "\n",
        "detection_list = []\n",
        "for key, value in detected_labels.items():\n",
        "    for detection in value:\n",
        "      # convert bounding box from x1/y1/x2/y2 format to [x1,y1],[x2,y1],[x2,y2],[x2,y1] coordinates format \n",
        "      bounding_box = np.array([[detection['box'][0], detection['box'][1]], [detection['box'][2], detection['box'][1]], [detection['box'][2], detection['box'][3]], [detection['box'][0], detection['box'][3]]])\n",
        "      # update the new coordinates format from local tile coordinates to orthomosaic coordinates\n",
        "      bounding_box = bounding_box + [img_data[\"image_locations\"][[key][0].split(\"/\")[-1]]]\n",
        "      # convert our bounding box from coordinates format back to x1/y1/x2/y2 format\n",
        "      bounding_box = [bounding_box[0][0], bounding_box[0][1], bounding_box[1][0], bounding_box[2][1]] \n",
        "      # update the dictionary\n",
        "      detection['box'] = bounding_box\n",
        "      #build our detection list. Note we no longer need filenames because the coordinates are no longer local\n",
        "      detection_list.append(detection)\n",
        "print(detection_list[0:3])"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'box': [8066, 4623, 8112, 4655], 'label': 1, 'score': 0.8100398778915405}, {'box': [8042, 4641, 8079, 4677], 'label': 1, 'score': 0.796779215335846}, {'box': [8037, 5080, 8086, 5156], 'label': 0, 'score': 0.7938922047615051}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4lajvLFICGO"
      },
      "source": [
        "# pull labels from classes.csv\n",
        "import csv\n",
        "with open(local_download_path + '/classes.csv', \"r\") as f:\n",
        "    reader = csv.reader(f, delimiter=\",\")\n",
        "    labels_to_names = {int(i[1]):i[0] for i in reader}\n",
        "\n",
        "boxes = []\n",
        "scores = []\n",
        "for detection in detection_list:\n",
        "  boxes.append(detection['box'])\n",
        "  scores.append(detection['score'])"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXqKqRuViP0D"
      },
      "source": [
        "# Malisiewicz et al.\n",
        "# https://www.pyimagesearch.com/2015/02/16/faster-non-maximum-suppression-python/\n",
        "# import the necessary packages\n",
        "import numpy as np\n",
        "\n",
        "def non_max_suppression(boxes, probs, overlapThresh):\n",
        "    # if there are no boxes, return an empty list\n",
        "    if len(boxes) == 0:\n",
        "        return []\n",
        "\n",
        "    # if the bounding boxes are integers, convert them to floats -- this\n",
        "    # is important since we'll be doing a bunch of divisions\n",
        "    if boxes.dtype.kind == \"i\":\n",
        "        boxes = boxes.astype(\"float\")\n",
        "\n",
        "    # initialize the list of picked indexes\n",
        "    pick = []\n",
        "\n",
        "    # grab the coordinates of the bounding boxes\n",
        "    x1 = boxes[:, 0]\n",
        "    y1 = boxes[:, 1]\n",
        "    x2 = boxes[:, 2]\n",
        "    y2 = boxes[:, 3]\n",
        "\n",
        "    # compute the area of the bounding boxes and grab the indexes to sort\n",
        "    # (in the case that no probabilities are provided, simply sort on the\n",
        "    # bottom-left y-coordinate)\n",
        "    area = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
        "    idxs = y2\n",
        "\n",
        "    # if probabilities are provided, sort on them instead\n",
        "    if probs is not None:\n",
        "        idxs = probs\n",
        "\n",
        "    # sort the indexes\n",
        "    idxs = np.argsort(idxs)\n",
        "\n",
        "    # keep looping while some indexes still remain in the indexes list\n",
        "    while len(idxs) > 0:\n",
        "        # grab the last index in the indexes list and add the index value\n",
        "        # to the list of picked indexes\n",
        "        last = len(idxs) - 1\n",
        "        i = idxs[last]\n",
        "        pick.append(i)\n",
        "\n",
        "        # find the largest (x, y) coordinates for the start of the bounding\n",
        "        # box and the smallest (x, y) coordinates for the end of the bounding\n",
        "        # box\n",
        "        xx1 = np.maximum(x1[i], x1[idxs[:last]])\n",
        "        yy1 = np.maximum(y1[i], y1[idxs[:last]])\n",
        "        xx2 = np.minimum(x2[i], x2[idxs[:last]])\n",
        "        yy2 = np.minimum(y2[i], y2[idxs[:last]])\n",
        "\n",
        "        # compute the width and height of the bounding box\n",
        "        w = np.maximum(0, xx2 - xx1 + 1)\n",
        "        h = np.maximum(0, yy2 - yy1 + 1)\n",
        "\n",
        "        # compute the ratio of overlap\n",
        "        overlap = (w * h) / area[idxs[:last]]\n",
        "\n",
        "        # delete all indexes from the index list that have overlap greater\n",
        "        # than the provided overlap threshold\n",
        "        idxs = np.delete(idxs, np.concatenate(([last],\n",
        "            np.where(overlap > overlapThresh)[0])))\n",
        "\n",
        "    # return the index of the bounding boxes that were picked\n",
        "    return pick"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmY7ZCEvicfS",
        "outputId": "a2b484ee-e68d-4fad-a239-02909f68ab00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "bboxes = np.array(boxes)\n",
        "pick = non_max_suppression(bboxes, scores, 0.6)\n",
        "nms_detection_list = []\n",
        "for i in pick:\n",
        "  nms_detection_list.append(detection_list[i])\n",
        "print(\"Before NMS: \" + str(len(detection_list)) + \" files\")\n",
        "print(\"After NMS: \" + str(len(nms_detection_list)) + \" files\")"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before NMS: 132 files\n",
            "After NMS: 126 files\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCCJtJtcN69u"
      },
      "source": [
        "# if you run this section multiple times it will repeatedly (and incorrectly) transform your coordinates. Don't do it.\n",
        "geolocated_bb = []\n",
        "for detection in nms_detection_list:\n",
        "  bounding_box = np.array([[detection['box'][0], detection['box'][1]], [detection['box'][2], detection['box'][1]], [detection['box'][2], detection['box'][3]], [detection['box'][0], detection['box'][3]]]).astype(float)\n",
        "  count = 0\n",
        "  for point in bounding_box:\n",
        "    point = dataset.transform * point\n",
        "    bounding_box[count] = point\n",
        "    count += 1\n",
        "  detection['box'] = bounding_box\n",
        "print(nms_detection_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aptObZCDVULg",
        "outputId": "6f7231cd-2d50-4e0c-8e84-abbd85b0f3c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(nms_detection_list[0:3])"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'box': array([[ 292130.18141, 5099948.34892],\n",
            "       [ 292131.7264 , 5099948.34892],\n",
            "       [ 292131.7264 , 5099947.16323],\n",
            "       [ 292130.18141, 5099947.16323]]), 'label': 1, 'score': 0.9102174639701843}, {'box': array([[ 292140.3496 , 5099960.16989],\n",
            "       [ 292142.28982, 5099960.16989],\n",
            "       [ 292142.28982, 5099957.29549],\n",
            "       [ 292140.3496 , 5099957.29549]]), 'label': 0, 'score': 0.887717604637146}, {'box': array([[ 292103.44949, 5100016.40034],\n",
            "       [ 292106.00052, 5100016.40034],\n",
            "       [ 292106.00052, 5100014.10082],\n",
            "       [ 292103.44949, 5100014.10082]]), 'label': 0, 'score': 0.8841370940208435}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fKKpsQ5pVV7"
      },
      "source": [
        "# write out the detections as a shapefile\n",
        "\n",
        "from collections import OrderedDict\n",
        "import fiona\n",
        "from fiona.crs import from_epsg\n",
        "\n",
        "# Set output directory\n",
        "output_dir = 'shapefile_output'\n",
        "\n",
        "# create the dir if it doesn't already exist\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "# Define your schema as a polygon geom with a couple of fields\n",
        "schema = {\n",
        "    'geometry': 'Polygon',\n",
        "    'properties': OrderedDict([\n",
        "        ('ImageName', 'str'),\n",
        "        ('Detection', 'str')\n",
        "  ])\n",
        "}\n",
        "\n",
        "with fiona.open(output_dir + '/seal_detections.shp',\n",
        "    'w',\n",
        "    driver='ESRI Shapefile',\n",
        "    crs=dataset.crs,\n",
        "    schema=schema) as c:\n",
        "    \n",
        "    for num, polygon in enumerate(nms_detection_list):\n",
        "      record = {\n",
        "            'geometry': {'coordinates': [polygon['box']], 'type': 'Polygon'},\n",
        "            'id': num,\n",
        "            'properties': OrderedDict([('ImageName', orthomosaic_file),\n",
        "                                       ('Detection', labels_to_names[polygon['label']])\n",
        "                                       ]),\n",
        "            'type': 'Feature'}\n",
        "      c.write(record)"
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKfVjeklm1rt"
      },
      "source": [
        "### Zip output folder for download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "aAmS92vcm1rt",
        "outputId": "023cb618-548b-4295-a8f9-77a7f42c91aa"
      },
      "source": [
        "# zip up the output directory into an archive for download\n",
        "import subprocess\n",
        "subprocess.call(['zip', '-r', '/content/' + output_dir + '.zip', '/content/' + output_dir])\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"/content/\" + output_dir + \".zip\")"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_7f5230a2-710e-4aa5-a5a6-b4e543dfca17\", \"shapefile_output.zip\", 7304)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jb_yWaChpVV_"
      },
      "source": [
        "### RetinaNet to Existing VIA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxZjZKbfpVV_"
      },
      "source": [
        "# open the previous training data\n",
        "\n",
        "with open(local_download_path + '/via_SealCNN_TrainingData.json') as f:\n",
        "    existing_labels = json.load(f)\n",
        "\n",
        "# add the new detections to the old via_region_data.json file\n",
        "\n",
        "#\"11_fiX1mEhK\",\"[\"\"2015_02_02_hay_island_flight03_s110rgb_jpeg_mosaic_group1---27.png\"\"]\",0,\"[]\",\"[2,509.275,929.174,89.376,48.904]\",\"{}\"\n",
        "\n",
        "for filepath, detections in detected_labels.items():\n",
        "    fn = filepath.split(\"/\")[-1]\n",
        "    # TODO is deep copy correct?\n",
        "    for filename_size, metadata in existing_labels.items():\n",
        "        if fn == metadata[\"filename\"]:\n",
        "            for detection in detections:\n",
        "                # 'box' : [x1, y1, x2, y2]\n",
        "                x1 = detection[\"box\"][0]\n",
        "                y1 = detection[\"box\"][1]\n",
        "                x2 = detection[\"box\"][2]\n",
        "                y2 = detection[\"box\"][3]\n",
        "                #print(x1,x2,y1,y2)\n",
        "                metadata[\"regions\"].append({'shape_attributes': {'name': 'rect', 'x': x1, 'y': y1, 'width': x2-x1, 'height': y2-y1}, 'region_attributes': {}})\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioY92hB7pVWB"
      },
      "source": [
        "# write out new VIA file with additional detections\n",
        "\n",
        "with open(local_download_path + '/via_region_data_detections.json', 'w') as fp:\n",
        "    json.dump(existing_labels, fp)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}