{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "export_outputs.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8WTbTOrpVVf"
      },
      "source": [
        "## Auditing and Exporting Detections"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o97tafnarHqG"
      },
      "source": [
        "**Before running this script, make sure that your Google Drive folder contains  the `tiling_scheme.json` file (`step 1`), the `classes.csv` and `subset_list` files (`step3`) and the `new_detections.json` file (`step 4`).**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QI6JfXBBm1rs"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gl7176/GreySealCNN/blob/master/5_export_outputs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "<center> Be sure to update this hyperlink above if you clone and want to point to a different GitHub </center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0SGgVrGEWZN"
      },
      "source": [
        "### Connect to our Google Drive folder and pull files\n",
        "Note: when you run this it will give you a link that you must click. You must give Google some permissions, then copy a code into a box that comes up in the output section of this code.\n",
        "\n",
        "If customizing this code, you will need to point the `drive_folder` variable to a URL for your shared google drive folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYEIuuoGpkZm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3dae4ce-dc0b-4b46-c01f-01fe5649c5fc"
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "import os, csv\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# choose a local (colab) directory to store the data.\n",
        "local_download_path = os.path.expanduser('data')\n",
        "try:\n",
        "  os.makedirs(local_download_path)\n",
        "except: pass\n",
        "\n",
        "# 2. Auto-iterate using the query syntax\n",
        "#    https://developers.google.com/drive/v2/web/search-parameters\n",
        "\n",
        "# set variable to the destination google drive folder you want to pull from\n",
        "drive_folder = 'https://drive.google.com/drive/folders/1INuRNVKvKMy8L_Nb6lmoVbyvScWK0-0D'\n",
        "\n",
        "# this bit points the code to that google drive folder\n",
        "pointer = str(\"'\" + drive_folder.split(\"/\")[-1] + \"'\" + \" in parents\")\n",
        "\n",
        "file_list = drive.ListFile(\n",
        "    {'q': pointer}).GetList()\n",
        "\n",
        "#    this bit pulls key files from the directory specified above\n",
        "#    and checks that all necessary files are present\n",
        "\n",
        "\n",
        "for f in file_list:\n",
        "  # 3. Create & download by id.\n",
        "  fname = os.path.join(local_download_path, f['title'])\n",
        "  if fname.endswith(\".json\") or fname.endswith(\".csv\"):\n",
        "    f_ = drive.CreateFile({'id': f['id']})\n",
        "    f_.GetContentFile(fname)\n",
        "    print(\"Pulled file: \" + fname)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pulled file: data/tiling_scheme.json\n",
            "Pulled file: data/new_detections.json\n",
            "Pulled file: data/classes.csv\n",
            "Pulled file: data/annotations_train.csv\n",
            "Pulled file: data/annotations_test.csv\n",
            "Pulled file: data/subset_list.csv\n",
            "Pulled file: data/annotations_valid.csv\n",
            "Pulled file: data/via_SealCNN_TrainingData.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zi5hhM923Uz1"
      },
      "source": [
        "### Identify necessary files from the input directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkenYfJx3Ufj",
        "outputId": "88be31cb-f993-4e93-9eba-4c03ac592017"
      },
      "source": [
        "# use this variable to set input directory\n",
        "input_dir = local_download_path\n",
        "\n",
        "tiling_scheme_file = 'tiling_scheme.json'\n",
        "classes_file = 'classes.csv'\n",
        "new_detections_file = 'new_detections.json'\n",
        "subset_list_file = 'subset_list.csv'\n",
        "annotations_file = 'annotations_placeholder'\n",
        "\n",
        "checklist = {tiling_scheme_file:\"tiling_scheme_file\",\n",
        "             classes_file:\"classes_file\", new_detections_file:\"new_detections_file\",\n",
        "             subset_list_file:\"subset_list_file\", annotations_file:\"annotations_file\"}\n",
        " \n",
        "for fname in os.listdir(input_dir):\n",
        "    candidate_file = \"{i}/{f}\".format(i=input_dir, f=fname)\n",
        "    os.stat(candidate_file)\n",
        "    if fname.endswith(\".csv\") or fname.endswith(\".json\"):\n",
        "      with open(candidate_file, \"r\") as f:\n",
        "        if next(csv.reader(f, delimiter=\",\"))[0:3] == ['filename', 'file_size', 'file_attributes']:\n",
        "          annotations_file = candidate_file\n",
        "          print(\"annotations file identified as \" + annotations_file)\n",
        "          del checklist['annotations_placeholder']\n",
        "        else:\n",
        "          try: \n",
        "            vars()[checklist[fname]] = \"{i}/{f}\".format(i=input_dir, f=fname)\n",
        "            print(\"required file found: {v}\".format(v=vars()[checklist[fname]]))\n",
        "            del checklist[fname]\n",
        "          except: print(\"{f} detected but not listed among requirements\".format(f=fname))\n",
        "\n",
        "if len(checklist) > 0:\n",
        "  for key in checklist:\n",
        "    print(\"Error: did not find {k} in your input folder\".format(k=key))\n",
        "  raise Exception(\"missing specified data files\")"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "annotations_valid.csv detected but not listed among requirements\n",
            "required file found: data/subset_list.csv\n",
            "annotations file identified as data/via_SealCNN_TrainingData.csv\n",
            "required file found: data/classes.csv\n",
            "required file found: data/tiling_scheme.json\n",
            "annotations_test.csv detected but not listed among requirements\n",
            "required file found: data/new_detections.json\n",
            "annotations_train.csv detected but not listed among requirements\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHksaZEPEWZO"
      },
      "source": [
        "### Set up the python environment and key variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biwSnxfdpVVj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56bc0c15-6566-4f07-bd0e-95de195b332d"
      },
      "source": [
        "import os\n",
        "import argparse\n",
        "import numpy as np\n",
        "import json\n",
        "import csv\n",
        "!pip install rasterio==1.1.8\n",
        "import rasterio\n",
        "import copy\n",
        "from collections import OrderedDict\n",
        "!pip install fiona\n",
        "import fiona # only required for exporting to shapefiles\n",
        "from fiona.crs import from_epsg\n",
        "from shapely.geometry import mapping, Polygon\n",
        "\n",
        "# make a dictionary where {filename} calls {subset type}\n",
        "with open(subset_list_file, \"r\") as f:\n",
        "    reader = csv.reader(f, delimiter=\",\")\n",
        "    subset_dict = {i[0]:i[1] for i in reader}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting rasterio==1.1.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/09/5a/92fd30ae3addd07ae71d865665bebc29feed99dc48d86a58bcb38b03c5f1/rasterio-1.1.8-1-cp37-cp37m-manylinux1_x86_64.whl (18.3MB)\n",
            "\u001b[K     |████████████████████████████████| 18.3MB 247kB/s \n",
            "\u001b[?25hRequirement already satisfied: click<8,>=4.0 in /usr/local/lib/python3.7/dist-packages (from rasterio==1.1.8) (7.1.2)\n",
            "Collecting cligj>=0.5\n",
            "  Downloading https://files.pythonhosted.org/packages/42/1e/947eadf10d6804bf276eb8a038bd5307996dceaaa41cfd21b7a15ec62f5d/cligj-0.7.1-py3-none-any.whl\n",
            "Collecting snuggs>=1.4.1\n",
            "  Downloading https://files.pythonhosted.org/packages/cc/0e/d27d6e806d6c0d1a2cfdc5d1f088e42339a0a54a09c3343f7f81ec8947ea/snuggs-1.4.7-py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rasterio==1.1.8) (1.19.5)\n",
            "Collecting click-plugins\n",
            "  Downloading https://files.pythonhosted.org/packages/e9/da/824b92d9942f4e472702488857914bdd50f73021efea15b4cad9aca8ecef/click_plugins-1.1.1-py2.py3-none-any.whl\n",
            "Collecting affine\n",
            "  Downloading https://files.pythonhosted.org/packages/ac/a6/1a39a1ede71210e3ddaf623982b06ecfc5c5c03741ae659073159184cd3e/affine-2.3.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from rasterio==1.1.8) (20.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.1.6 in /usr/local/lib/python3.7/dist-packages (from snuggs>=1.4.1->rasterio==1.1.8) (2.4.7)\n",
            "Installing collected packages: cligj, snuggs, click-plugins, affine, rasterio\n",
            "Successfully installed affine-2.3.0 click-plugins-1.1.1 cligj-0.7.1 rasterio-1.1.8 snuggs-1.4.7\n",
            "Collecting fiona\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/2a/404b22883298a3efe9c6ef8d67acbf2c38443fa366ee9cd4cd34e17626ea/Fiona-1.8.19-cp37-cp37m-manylinux1_x86_64.whl (15.3MB)\n",
            "\u001b[K     |████████████████████████████████| 15.3MB 7.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: click-plugins>=1.0 in /usr/local/lib/python3.7/dist-packages (from fiona) (1.1.1)\n",
            "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.7/dist-packages (from fiona) (0.7.1)\n",
            "Collecting munch\n",
            "  Downloading https://files.pythonhosted.org/packages/cc/ab/85d8da5c9a45e072301beb37ad7f833cd344e04c817d97e0cc75681d248f/munch-2.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six>=1.7 in /usr/local/lib/python3.7/dist-packages (from fiona) (1.15.0)\n",
            "Requirement already satisfied: click<8,>=4.0 in /usr/local/lib/python3.7/dist-packages (from fiona) (7.1.2)\n",
            "Requirement already satisfied: attrs>=17 in /usr/local/lib/python3.7/dist-packages (from fiona) (20.3.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from fiona) (2020.12.5)\n",
            "Installing collected packages: munch, fiona\n",
            "Successfully installed fiona-1.8.19 munch-2.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpWz8PqR4eCu"
      },
      "source": [
        "### Create some useful functions and variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dK9pBzIF4cbA"
      },
      "source": [
        "# takes a list and replaces dictionaries of {'box': [x1, y1, x2, y2]} in file-pixel coordinates\n",
        "# with dictionaries of {'box': array([[ 291942.12379, 5099949.2831 ], [ 291944.92633, 5099949.2831 ],\n",
        "# [291944.92633, 5099946.87579], [ 291942.12379, 5099946.87579]])} in global coordinates\n",
        "\n",
        "# pull labels from classes.csv\n",
        "import csv\n",
        "with open(classes_file, \"r\") as f:\n",
        "    reader = csv.reader(f, delimiter=\",\")\n",
        "    labels_to_names = {int(i[1]):i[0] for i in reader}\n",
        "\n",
        "# Set output directory, create it if necessary\n",
        "output_dir = 'shapefile_outputs'\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "# load parameters from tiling scheme\n",
        "with open(tiling_scheme_file) as f:\n",
        "    tiling_scheme = json.load(f)\n",
        "    orthomosaic_name = tiling_scheme[\"orthomosaic_file\"]\n",
        "    tile_height = tiling_scheme[\"tile_height\"]\n",
        "    tile_width = tiling_scheme[\"tile_width\"]\n",
        "    proj = tiling_scheme[\"spatial_reference\"]\n",
        "    gt = tiling_scheme[\"transform\"]\n",
        "    img_data = tiling_scheme[\"tile_pointers\"]\n",
        "\n",
        "# convert gt data list to a geotransform matrix\n",
        "from affine import Affine\n",
        "\n",
        "geotransform = (gt[2], gt[0], gt[1], gt[5], gt[3], gt[4])\n",
        "geotransform = Affine.from_gdal(*geotransform)\n",
        "\n",
        "# define function to transform coordinates from orthomosaic/pixel reference to global reference\n",
        "def global_transform(box_list):\n",
        "    for box in box_list:\n",
        "      bounding_box = np.array([[box['box'][0], box['box'][1]], [box['box'][2], box['box'][1]], [box['box'][2], box['box'][3]], [box['box'][0], box['box'][3]]]).astype(float)\n",
        "      count = 0\n",
        "      for point in bounding_box:\n",
        "        point = geotransform * point\n",
        "        bounding_box[count] = point\n",
        "        count += 1\n",
        "      box['box'] = bounding_box\n",
        "    print(box_list[0:3])\n",
        "\n",
        "\n",
        "# implement a non-max suppression function to remove redundant boxes\n",
        "# Malisiewicz et al.\n",
        "# https://www.pyimagesearch.com/2015/02/16/faster-non-maximum-suppression-python/\n",
        "def non_max_suppression(boxes, probs, overlapThresh):\n",
        "  # if there are no boxes, return an empty list\n",
        "  if len(boxes) == 0:\n",
        "      return []\n",
        "\n",
        "  # if the bounding boxes are integers, convert them to floats -- this\n",
        "  # is important since we'll be doing a bunch of divisions\n",
        "  if boxes.dtype.kind == \"i\":\n",
        "      boxes = boxes.astype(\"float\")\n",
        "\n",
        "  # initialize the list of picked indexes\n",
        "  pick = []\n",
        "\n",
        "  # grab the coordinates of the bounding boxes\n",
        "  x1, y1, x2, y2 = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]\n",
        "\n",
        "  # compute the area of the bounding boxes and grab the indexes to sort\n",
        "  # (in the case that no probabilities are provided, simply sort on the\n",
        "  # bottom-left y-coordinate)\n",
        "  area = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
        "  idxs = y2\n",
        "\n",
        "  # if probabilities are provided, sort on them instead\n",
        "  if probs is not None:\n",
        "      idxs = probs\n",
        "\n",
        "  # sort the indexes\n",
        "  idxs = np.argsort(idxs)\n",
        "\n",
        "  # keep looping while some indexes still remain in the indexes list\n",
        "  while len(idxs) > 0:\n",
        "      # grab the last index in the indexes list and add the index value\n",
        "      # to the list of picked indexes\n",
        "      last = len(idxs) - 1\n",
        "      i = idxs[last]\n",
        "      pick.append(i)\n",
        "\n",
        "      # find the largest (x, y) coordinates for the start of the bounding\n",
        "      # box and the smallest (x, y) coordinates for the end of the bounding\n",
        "      # box\n",
        "      xx1, yy1, xx2, yy2 = np.maximum(x1[i], x1[idxs[:last]]), np.maximum(y1[i], y1[idxs[:last]]), np.minimum(x2[i], x2[idxs[:last]]), np.minimum(y2[i], y2[idxs[:last]])\n",
        "\n",
        "      # compute the width and height of the bounding box\n",
        "      w, h = np.maximum(0, xx2 - xx1 + 1), np.maximum(0, yy2 - yy1 + 1)\n",
        "\n",
        "      # compute the ratio of overlap\n",
        "      overlap = (w * h) / area[idxs[:last]]\n",
        "\n",
        "      # delete all indexes from the index list that have overlap greater\n",
        "      # than the provided overlap threshold\n",
        "      idxs = np.delete(idxs, np.concatenate(([last],\n",
        "          np.where(overlap > overlapThresh)[0])))\n",
        "\n",
        "  # return the index of the bounding boxes that were picked\n",
        "  return pick"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xrjsw-07pVVz"
      },
      "source": [
        "### Convert detections from image-/tile-based coordinates to orthomosaic coordinates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIisHMg-7CGj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82d81f6d-71e9-4ff4-f793-e6176904ce36"
      },
      "source": [
        "# open the output from our CNN\n",
        "with open(new_detections_file) as f:\n",
        "    detected_labels = json.load(f)\n",
        "\n",
        "# update box locations from local tile coordinates to orthomosaic coordinates\n",
        "# using image_locations from original tiling process, invoked by filename\n",
        "# ---this involves transforming x1, y1, x2, y2 coordinates to 4 sets of x,y points\n",
        "# and then transforming them back to the original form---\n",
        "# then build a list of detection dictionaries. Note that filename is no longer\n",
        "# needed because coordinates are now relative to the orthomosaic, not tile\n",
        "\n",
        "detection_list = []\n",
        "for key, value in detected_labels.items():\n",
        "    for detection in value:\n",
        "      # convert bounding box from x1/y1/x2/y2 format to [x1,y1],[x2,y1],[x2,y2],[x2,y1] coordinates format \n",
        "      bounding_box = np.array([[detection['box'][0], detection['box'][1]], [detection['box'][2], detection['box'][1]], [detection['box'][2], detection['box'][3]], [detection['box'][0], detection['box'][3]]])\n",
        "      # update the new coordinates format from local tile coordinates to orthomosaic coordinates\n",
        "      bounding_box = bounding_box + [img_data[\"image_locations\"][[key][0].split(\"/\")[-1]]]\n",
        "      # convert our bounding box from coordinates format back to x1/y1/x2/y2 format\n",
        "      bounding_box = [bounding_box[0][0], bounding_box[0][1], bounding_box[1][0], bounding_box[2][1]] \n",
        "      # update the dictionary\n",
        "      detection['box'] = bounding_box\n",
        "      #build our detection list. Note we no longer need filenames because the coordinates are no longer local\n",
        "      detection_list.append(detection)\n",
        "print(detection_list[0:3])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'box': [9162, 5189, 9229, 5243], 'label': 0, 'score': 0.9904279708862305}, {'box': [8561, 4868, 8640, 4914], 'label': 0, 'score': 0.9815141558647156}, {'box': [8959, 5462, 9024, 5523], 'label': 0, 'score': 0.9747313261032104}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDAVvdbW0Hy0"
      },
      "source": [
        "### Implement non-max suppression on duplicate CNN detections"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmY7ZCEvicfS",
        "outputId": "64d05e67-d2f4-47dc-8649-aa7bad88a5a7"
      },
      "source": [
        "boxes = []\n",
        "scores = []\n",
        "for detection in detection_list:\n",
        "  boxes.append(detection['box'])\n",
        "  scores.append(detection['score'])\n",
        "  \n",
        "bboxes = np.array(boxes)\n",
        "pick = non_max_suppression(bboxes, scores, 0.6)\n",
        "nms_detection_list = []\n",
        "for i in pick:\n",
        "  nms_detection_list.append(detection_list[i])\n",
        "\n",
        "# backup the nms_detection_list before running next section, so if something\n",
        "# goes wrong we don't need to re-run the whole code\n",
        "nms_detection_list_backup = copy.deepcopy(nms_detection_list)\n",
        "\n",
        "print(\"Before NMS: \" + str(len(detection_list)) + \" detections\")\n",
        "print(\"After NMS: \" + str(len(nms_detection_list)) + \" detections\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before NMS: 424 detections\n",
            "After NMS: 396 detections\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuyAwIh11qes"
      },
      "source": [
        "### Export CNN detections shapefile"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCCJtJtcN69u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "221c6276-6484-434f-9748-2eb67625d46d"
      },
      "source": [
        "# the global transform function permanently alters nms_detection_list, necessitating the deepcopy backup earlier\n",
        "nms_detection_list = copy.deepcopy(nms_detection_list_backup)\n",
        "global_transform(nms_detection_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'box': array([[ 292242.78603, 5099983.91962],\n",
            "       [ 292245.22927, 5099983.91962],\n",
            "       [ 292245.22927, 5099981.94347],\n",
            "       [ 292242.78603, 5099981.94347]]), 'label': 0, 'score': 0.9973013997077942}, {'box': array([[ 292221.76698, 5100208.26654],\n",
            "       [ 292224.28208, 5100208.26654],\n",
            "       [ 292224.28208, 5100206.1826 ],\n",
            "       [ 292221.76698, 5100206.1826 ]]), 'label': 0, 'score': 0.9951584935188293}, {'box': array([[ 292035.86516, 5100092.50008],\n",
            "       [ 292038.84735, 5100092.50008],\n",
            "       [ 292038.84735, 5100091.13474],\n",
            "       [ 292035.86516, 5100091.13474]]), 'label': 0, 'score': 0.99210524559021}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fKKpsQ5pVV7"
      },
      "source": [
        "# Define your schema as a polygon geom with a couple of fields\n",
        "# then write out the detections as a shapefile\n",
        "schema = {\n",
        "    'geometry': 'Polygon',\n",
        "    'properties': OrderedDict([\n",
        "        ('ImageName', 'str'),\n",
        "        ('Detection', 'str'),\n",
        "        ('Score', 'float')\n",
        "  ])\n",
        "}\n",
        "\n",
        "with fiona.open(output_dir + '/seal_detections.shp',\n",
        "    'w',\n",
        "    driver='ESRI Shapefile',\n",
        "    crs=rasterio.crs.CRS.from_dict(init=proj),\n",
        "    schema=schema) as c:\n",
        "    \n",
        "    for num, polygon in enumerate(nms_detection_list):\n",
        "      record = {\n",
        "            'geometry': {'coordinates': [polygon['box']], 'type': 'Polygon'},\n",
        "            'id': num,\n",
        "            'properties': OrderedDict([('ImageName', orthomosaic_name),\n",
        "                                       ('Detection', labels_to_names[polygon['label']]),\n",
        "                                       ('Score', polygon['score'])\n",
        "                                       ]),\n",
        "            'type': 'Feature'}\n",
        "      c.write(record)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZX8_fY9CWX3R"
      },
      "source": [
        "#Exporting original annotations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D98r_mFO4ooy"
      },
      "source": [
        "### Format original VIA annotations to necessary information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3d8-GkH4nqb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5b3acc7-d537-4229-b7f8-b1924fa71599"
      },
      "source": [
        "via_annotations_list = []\n",
        "\n",
        "# read each line, parse it, convert it, put it all back together\n",
        "# then drop it in the appropriate subset\n",
        "with open(annotations_file, \"r\") as f:\n",
        "    reader = csv.reader(f, delimiter=\",\")\n",
        "    for line in reader: \n",
        "        # output we want:\n",
        "        # format: path/to/image.jpg,x1,y1,x2,y2,class_name\n",
        "        # example: /data/imgs/img_001.jpg,837,346,981,456,cow\n",
        "        if 'filename' in line[0]:\n",
        "            # bypassing comments in csv\n",
        "            continue\n",
        "        if '{}' in line[5]:\n",
        "            #bypassing empty images\n",
        "            continue\n",
        "            \n",
        "        filename = line[0]\n",
        "        \n",
        "        # pulling from column named \"region_shape_attributes\"\n",
        "        box_entry = json.loads(line[5])\n",
        "        top_left_x, top_left_y, width, height = box_entry[\"x\"], box_entry[\"y\"], box_entry[\"width\"], box_entry[\"height\"]\n",
        "        if width == 0 or height == 0:\n",
        "            continue\n",
        "            # skip tiny/empty boxes\n",
        "        \n",
        "        # define area for NMS ranking later\n",
        "        area = width * height\n",
        "\n",
        "        # convert from \"top left and width/height\" to \"x and y values at each corner of the box\"\n",
        "        if top_left_x < 0: top_left_x = 1\n",
        "        if top_left_y < 0: top_left_y = 1\n",
        "        x1, x2, y1, y2 = top_left_x, top_left_x + width, top_left_y, top_left_y + height \n",
        "        \n",
        "        # pulling from column named \"region_attributes\" to get class names\n",
        "        name = json.loads(line[6])[\"Age Class\"]\n",
        "\n",
        "        # skip unknown class, in this case. Might be useful in other applications though, e.g. total count\n",
        "        if name == \"Unknown\":\n",
        "            continue\n",
        "        \n",
        "        # pull subset from dictionary\n",
        "        subset_type = subset_dict[filename]\n",
        "\n",
        "        # create the annotation row\n",
        "        new_row = [filename, [x1,y1], [x2,y1], [x2,y2], [x1,y2], name, area, subset_type]\n",
        "\n",
        "        # append the row to the our list\n",
        "        via_annotations_list.append(new_row)\n",
        "print(via_annotations_list[0:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['2015_02_02_hay_island_flight03_s110rgb_jpeg_mosaic_group1---28.png', [615, 927], [681, 927], [681, 959], [615, 959], 'Adult', 2112, 'training'], ['2015_02_02_hay_island_flight03_s110rgb_jpeg_mosaic_group1---28.png', [959, 917], [998, 917], [998, 943], [959, 943], 'Pup', 1014, 'training'], ['2015_02_02_hay_island_flight03_s110rgb_jpeg_mosaic_group1---28.png', [632, 899], [668, 899], [668, 932], [632, 932], 'Pup', 1188, 'training'], ['2015_02_02_hay_island_flight03_s110rgb_jpeg_mosaic_group1---28.png', [514, 937], [581, 937], [581, 968], [514, 968], 'Adult', 2077, 'training'], ['2015_02_02_hay_island_flight03_s110rgb_jpeg_mosaic_group1---29.png', [152, 671], [192, 671], [192, 697], [152, 697], 'Pup', 1040, 'training']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_55Fs6aLrp4_"
      },
      "source": [
        "### Convert via annotations from image-pixel coordinates to orthomosaic-pixel coordinates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkmJg5ivJEk2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b85255c-5082-4427-f4bc-5f05bd64496e"
      },
      "source": [
        "# give each detection a score for NMS, based on its area and prioritized by\n",
        "# testing > training datasets so during NMS if we encounter duplicates we prioritize\n",
        "# testing data for accuracy evaluation, and within that prioritization, we prioritize\n",
        "# larger boxes over smaller boxes (in case any \"edge\" animals were duplicated)\n",
        "area_max = max(list(i[6] for i in via_annotations_list))\n",
        "scores = []\n",
        "boxes = []\n",
        "\n",
        "detection_list = []\n",
        "for i in via_annotations_list:\n",
        "  score = i[6]/area_max\n",
        "  if i[7] != 'testing':\n",
        "    score = 0.01 * score\n",
        "  scores.append(score)\n",
        "\n",
        "  bounding_box = np.array([i[1],i[2],i[3],i[4]])\n",
        "  # update the new coordinates format from local tile coordinates to orthomosaic coordinates\n",
        "  bounding_box = bounding_box + [img_data[\"image_locations\"][i[0]]]\n",
        "  # convert our bounding box from coordinates format back to x1/y1/x2/y2 format\n",
        "  bounding_box = [bounding_box[0][0], bounding_box[0][1], bounding_box[1][0], bounding_box[2][1]] \n",
        "  boxes.append(bounding_box)\n",
        "\n",
        "  # update the dictionary\n",
        "  detection = {\"box\":bounding_box, \"score\":score, \"label\":i[5], \"subset\":i[7]}\n",
        "  detection_list.append(detection)\n",
        "print(detection_list[0:3])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'box': [9815, 1847, 9881, 1879], 'score': 0.003300515705578997, 'label': 'Adult', 'subset': 'training'}, {'box': [10159, 1837, 10198, 1863], 'score': 0.001584622597280825, 'label': 'Pup', 'subset': 'training'}, {'box': [9832, 1819, 9868, 1852], 'score': 0.0018565400843881857, 'label': 'Pup', 'subset': 'training'}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z82mHRx1r6jU"
      },
      "source": [
        "### Implement NMS on VIA annotations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tz8Cb8RM3izp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eba05b1c-1ecb-4893-af53-b56a8b15a2ef"
      },
      "source": [
        "bboxes = np.array(boxes)\n",
        "pick = non_max_suppression(bboxes, scores, 0.6)\n",
        "nms_detection_list = []\n",
        "for i in pick:\n",
        "  nms_detection_list.append(detection_list[i])\n",
        "\n",
        "# backup the nms_detection_list before running next section, so if something\n",
        "# goes wrong we don't need to re-run the whole code\n",
        "nms_detection_list_backup = copy.deepcopy(nms_detection_list)\n",
        "\n",
        "print(\"Before NMS: \" + str(len(detection_list)) + \" detections\")\n",
        "print(\"After NMS: \" + str(len(nms_detection_list)) + \" detections\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before NMS: 2808 detections\n",
            "After NMS: 2461 detections\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CVjR0cp4keA"
      },
      "source": [
        "### Export VIA annotations shapefile"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4W9evVO4kQRQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4c4fdde-4d02-4014-9b51-0cf74eec3372"
      },
      "source": [
        "nms_detection_list = copy.deepcopy(nms_detection_list_backup)\n",
        "global_transform(nms_detection_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'box': array([[ 292187.09453, 5100119.30386],\n",
            "       [ 292189.10661, 5100119.30386],\n",
            "       [ 292189.10661, 5100116.10609],\n",
            "       [ 292187.09453, 5100116.10609]]), 'score': 0.7788716987029223, 'label': 'Adult', 'subset': 'testing'}, {'box': array([[ 292220.61722, 5100189.97817],\n",
            "       [ 292222.66523, 5100189.97817],\n",
            "       [ 292222.66523, 5100187.03191],\n",
            "       [ 292220.61722, 5100187.03191]]), 'score': 0.7304266291608064, 'label': 'Adult', 'subset': 'testing'}, {'box': array([[ 292103.3417 , 5099948.16927],\n",
            "       [ 292106.61133, 5099948.16927],\n",
            "       [ 292106.61133, 5099946.33684],\n",
            "       [ 292103.3417 , 5099946.33684]]), 'score': 0.7252695733708392, 'label': 'Adult', 'subset': 'testing'}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYlR1CrB4kea"
      },
      "source": [
        "# write out the detections as a shapefile\n",
        "# Define your schema as a polygon geom with a couple of fields\n",
        "schema = {\n",
        "    'geometry': 'Polygon',\n",
        "    'properties': OrderedDict([\n",
        "        ('ImageName', 'str'),\n",
        "        ('Detection', 'str'),\n",
        "        ('Subset', 'str'),\n",
        "  ])\n",
        "}\n",
        "\n",
        "with fiona.open(output_dir + '/via_annotations.shp',\n",
        "    'w',\n",
        "    driver='ESRI Shapefile',\n",
        "    crs=rasterio.crs.CRS.from_dict(init=proj),\n",
        "    schema=schema) as c:\n",
        "    \n",
        "    for num, polygon in enumerate(nms_detection_list):\n",
        "      record = {\n",
        "            'geometry': {'coordinates': [polygon['box']], 'type': 'Polygon'},\n",
        "            'id': num,\n",
        "            'properties': OrderedDict([('ImageName', orthomosaic_name),\n",
        "                                       ('Detection', polygon['label']),\n",
        "                                       ('Subset', polygon['subset'])\n",
        "                                       ]),\n",
        "            'type': 'Feature'}\n",
        "      c.write(record)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oouXv6KiWheo"
      },
      "source": [
        "# Exporting subset regions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zt4rgzG7Xw60"
      },
      "source": [
        "### Convert tile spatial data to boxes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DONJjEmWnTJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1c4ab98-3809-415f-a2e8-422f7d70d98e"
      },
      "source": [
        "tile_list = []\n",
        "for i in img_data[\"image_locations\"]:\n",
        "  x1, y1 = img_data[\"image_locations\"][i]\n",
        "  x2, y2 = x1 + tile_width, y1 + tile_height\n",
        "  tile_square = {'box': [x1, y1, x2, y2], 'file_name':i, 'subset':subset_dict[i]}\n",
        "  tile_list.append(tile_square)\n",
        "global_transform(tile_list)\n",
        "\n",
        "# get it to x1/y1/x2/y2 format\n",
        "\n",
        "# takes a list and replaces dictionaries of {'box': [x1, y1, x2, y2]} in file-pixel coordinates\n",
        "# with dictionaries of {'box': array([[ 291942.12379, 5099949.2831 ], [ 291944.92633, 5099949.2831 ],\n",
        "# [291944.92633, 5099946.87579], [ 291942.12379, 5099946.87579]])} in global coordinates\n",
        "\n",
        "# def global_transform(box_list, geo_reference_file):"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'box': array([[ 292135.35533, 5100352.95665],\n",
            "       [ 292171.28533, 5100352.95665],\n",
            "       [ 292171.28533, 5100317.02665],\n",
            "       [ 292135.35533, 5100317.02665]]), 'file_name': '2015_02_02_hay_island_flight03_s110rgb_jpeg_mosaic_group1---9.png', 'subset': 'training'}, {'box': array([[ 292168.41093, 5100352.95665],\n",
            "       [ 292204.34093, 5100352.95665],\n",
            "       [ 292204.34093, 5100317.02665],\n",
            "       [ 292168.41093, 5100317.02665]]), 'file_name': '2015_02_02_hay_island_flight03_s110rgb_jpeg_mosaic_group1---10.png', 'subset': 'training'}, {'box': array([[ 292201.46653, 5100352.95665],\n",
            "       [ 292237.39653, 5100352.95665],\n",
            "       [ 292237.39653, 5100317.02665],\n",
            "       [ 292201.46653, 5100317.02665]]), 'file_name': '2015_02_02_hay_island_flight03_s110rgb_jpeg_mosaic_group1---11.png', 'subset': 'training'}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9l89wGvsAtc"
      },
      "source": [
        "Export tile footprints as shapefile"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6g-x1Z-lJXu"
      },
      "source": [
        "# Define your schema as a polygon geom with a couple of fields\n",
        "schema = {\n",
        "    'geometry': 'Polygon',\n",
        "    'properties': OrderedDict([\n",
        "        ('ImageName', 'str'),\n",
        "        ('Subset', 'str'),\n",
        "  ])\n",
        "}\n",
        "\n",
        "with fiona.open(output_dir + '/tile_footprints.shp',\n",
        "    'w',\n",
        "    driver='ESRI Shapefile',\n",
        "    crs=rasterio.crs.CRS.from_dict(init=proj),\n",
        "    schema=schema) as c:\n",
        "    \n",
        "    for num, polygon in enumerate(tile_list):\n",
        "      record = {\n",
        "            'geometry': {'coordinates': [polygon['box']], 'type': 'Polygon'},\n",
        "            'id': num,\n",
        "            'properties': OrderedDict([('ImageName', polygon['file_name']),\n",
        "                                       ('Subset', polygon['subset'])\n",
        "                                       ]),\n",
        "            'type': 'Feature'}\n",
        "      c.write(record)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_ULh0LY4keb"
      },
      "source": [
        "# Zip output folder for download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mLJIsK_4kec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 16
        },
        "outputId": "6d693a2a-2241-41b6-a23e-ef11e22cb650"
      },
      "source": [
        "# zip up the output directory into an archive for download\n",
        "output_file_name = 'Step_5_{o}'.format(o=output_dir)\n",
        "import subprocess\n",
        "subprocess.call(['zip', '-r', output_file_name + '.zip', '/content/' + output_dir])\n",
        "\n",
        "from google.colab import files\n",
        "files.download(output_file_name + \".zip\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_39491500-44ec-4c9f-aa65-199c0da2089b\", \"Step_5_shapefile_outputs.zip\", 141490)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}