{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the CNN + model, Train it and Test it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Before running this script, make sure that your Google Drive folder contains the tiles you created (`step 1`) and the 5 CSVs that you created (`step 3`) to describe the 3 data subsets (1 CSV file), the annotations for each (3 CSV files) and the class list (1 CSV file)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/gl7176/GreySealCNN/blob/master/4_CNN_setup_training_testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "#####  <center> Be sure to update this hyperlink above if you clone and want to point to a different GitHub </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8XElcTShEQkh"
   },
   "source": [
    "### Connect to our Google Drive folder and pull all data\n",
    "Note: when you run this it will give you a link that you must click. You must give Google some permissions, then copy a code into a box that comes up in the output section of this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "le3bfsAjzSP_"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-e2b2c84ee6a7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpydrive\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauth\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGoogleAuth\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpydrive\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrive\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGoogleDrive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mauth\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0moauth2client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGoogleCredentials\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "!pip install -U -q PyDrive\n",
    "import os\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "# 1. Authenticate and create the PyDrive client.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "\n",
    "# choose a local (colab) directory to store the data.\n",
    "local_download_path = os.path.expanduser('data')\n",
    "try:\n",
    "  os.makedirs(local_download_path)\n",
    "except: pass\n",
    "\n",
    "# 2. Auto-iterate using the query syntax\n",
    "#    https://developers.google.com/drive/v2/web/search-parameters\n",
    "#    The mess of characters before \"in parents\" is the address of the google drive folder getting pulled\n",
    "#    (when you open the folder in google drive on a browser, it is the mess of characters at the end of the URL)\n",
    "#    for example, it currently points to: https://drive.google.com/drive/folders/1INuRNVKvKMy8L_Nb6lmoVbyvScWK0-0D\n",
    "\n",
    "file_list = drive.ListFile(\n",
    "    {'q': \"'1INuRNVKvKMy8L_Nb6lmoVbyvScWK0-0D' in parents\"}).GetList()\n",
    "\n",
    "#    this bit pulls every file in the directory specified above\n",
    "count = 0\n",
    "for f in file_list:\n",
    "  count += 1\n",
    "  if count % 10 == 0:\n",
    "    print(count)\n",
    "  # 3. Create & download by id.\n",
    "  fname = os.path.join(local_download_path, f['title'])\n",
    "  f_ = drive.CreateFile({'id': f['id']})\n",
    "  f_.GetContentFile(fname)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n-xdfna11L2F"
   },
   "source": [
    "### Install the Convolutional Neural Network that will do the detections. \n",
    "\n",
    "This section sets up the software and pulls code for a CNN model called \"RetinaNet\" which uses the model \"ResNet-50\" as a subcomponent. This section then loads data for an existing ResNet-50 model (pre-trained for object detection) which we will further train for our task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N-VXXOfd-LXa"
   },
   "outputs": [],
   "source": [
    "# clear any existing builds and install the versions that work for us\n",
    "! pip uninstall --yes keras\n",
    "! pip install keras==2.4\n",
    "! pip uninstall --yes tensorflow\n",
    "! pip install tensorflow==2.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_jb7YRXlEUUg"
   },
   "outputs": [],
   "source": [
    "# copy the files for RetinaNet\n",
    "# note that this build is now deprecated, but we are fine with that\n",
    "! git clone https://github.com/fizyr/keras-retinanet/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RuULO44w6yx8"
   },
   "outputs": [],
   "source": [
    "# change directory and install RetinaNet from the copied code\n",
    "% cd keras-retinanet\n",
    "\n",
    "! pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uu_IOPbC44SC"
   },
   "outputs": [],
   "source": [
    "! python setup.py build_ext --inplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "33S3M0SoI1JI"
   },
   "outputs": [],
   "source": [
    "% cd ../\n",
    "\n",
    "# get the pre-trained ResNet-50 model\n",
    "! wget -P data \"https://github.com/fizyr/keras-retinanet/releases/download/0.5.1/resnet50_coco_best_v2.1.0.h5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LYBUnE6wEKuq"
   },
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yzdoAR2GEKuv"
   },
   "source": [
    "We're giving our model the pre-trained weights that we downloaded above, and then we're telling it to use the training data in annotations_train.csv and to run for `10 epochs` each with `20 steps` with a `batch-size of 2`. An epoch is a group of steps after which the model calculates its accuracy; a step is an increment of training the model on one batch or subset of files.\n",
    "\n",
    "For real applications, we're going to want to step up the epoch and step counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mcn229SrEKuw"
   },
   "outputs": [],
   "source": [
    "! keras-retinanet/keras_retinanet/bin/train.py \\\n",
    "--weights data/resnet50_coco_best_v2.1.0.h5 \\\n",
    "--epochs 10 --steps 20 --batch-size 2 \\\n",
    "csv data/annotations_train.csv data/classes.csv \\\n",
    "--val-annotations data/annotations_valid.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "apHCQjSo8WC2"
   },
   "source": [
    "This next section converts the model from training mode to inference mode so it can be used to detection seals. This conversion process take a little time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8YyMralKEKuz"
   },
   "outputs": [],
   "source": [
    "# note that we are naming our model \"test_model\" and locating it in the \"snapshots\" directory. Customize if wanted\n",
    "! keras-retinanet/keras_retinanet/bin/convert_model.py snapshots/resnet50_csv_10.h5 snapshots/test_model.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vu3yk48fEKu2"
   },
   "source": [
    "### Run Detection in inference mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GbTLLmbGEKu2"
   },
   "source": [
    "This section sets up the environment, importing modules for python tasks and specific to keras+retinanet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bj-WmUXiEKu3"
   },
   "outputs": [],
   "source": [
    "# show images inline\n",
    "%matplotlib inline\n",
    "\n",
    "# automatically reload modules when they have changed\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import keras\n",
    "import keras\n",
    "\n",
    "# import keras_retinanet\n",
    "from keras_retinanet import models\n",
    "from keras_retinanet.utils.image import read_image_bgr, preprocess_image, resize_image\n",
    "from keras_retinanet.utils.visualization import draw_box, draw_caption\n",
    "from keras_retinanet.utils.colors import label_color\n",
    "\n",
    "# import miscellaneous modules\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "from random import shuffle\n",
    "\n",
    "# set tf backend to allow memory to grow, instead of claiming everything\n",
    "import tensorflow as tf\n",
    "\n",
    "def get_session():\n",
    "    config = tf.compat.v1.ConfigProto()    \n",
    "    config.gpu_options.allow_growth = True\n",
    "    return tf.compat.v1.Session(config=config)\n",
    "\n",
    "# use this environment flag to change which GPU to use\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "# set the modified tf session as backend in keras\n",
    "tf.compat.v1.keras.backend.set_session(get_session())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w987GUwIEKu5"
   },
   "source": [
    "### Load RetinaNet model\n",
    "\n",
    "Now we will load the model that you just converted into inference mode: it is called `test_model.h5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VDo883mYEKu6"
   },
   "outputs": [],
   "source": [
    "model_path = 'snapshots/test_model.h5'\n",
    "\n",
    "print(model_path)\n",
    "\n",
    "# load retinanet model\n",
    "model = models.load_model(model_path, backbone_name='resnet50')\n",
    "\n",
    "# you can check out a model summary by uncommenting this line\n",
    "#print(model.summary())\n",
    "\n",
    "# load label to names mapping for visualization purposes\n",
    "# pull labels from classes.csv\n",
    "import csv\n",
    "with open(local_download_path + '/classes.csv', \"r\") as f:\n",
    "    reader = csv.reader(f, delimiter=\",\")\n",
    "    labels_to_names = {int(i[1]):i[0] for i in reader}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2jfMnDLVEKu9"
   },
   "source": [
    "### Load test imagery\n",
    "\n",
    "Now we will load the \"test\" subset of images that we downloaded into our data directory during setup (as listed in the subsets CSV file). Just to check, we'll print out the first five names of those images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7QjtUN5wEKu9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/2015_02_02_hay_island_flight03_s110rgb_jpeg_mosaic_group1---159.png', 'data/2015_02_02_hay_island_flight03_s110rgb_jpeg_mosaic_group1---193.png', 'data/2015_02_02_hay_island_flight03_s110rgb_jpeg_mosaic_group1---27.png', 'data/2015_02_02_hay_island_flight03_s110rgb_jpeg_mosaic_group1---212.png', 'data/2015_02_02_hay_island_flight03_s110rgb_jpeg_mosaic_group1---94.png']\n"
     ]
    }
   ],
   "source": [
    "# load imagery\n",
    "image_dir = \"data/\"\n",
    "\n",
    "# this code pulls only files from the test or validation subset\n",
    "# as specified in this variable, \"target_subset\" either \"test\" or \"validation\"\n",
    "target_subset = \"test\"\n",
    "\n",
    "image_list = []\n",
    "with open(local_download_path + '/subset_list.csv', \"r\") as f:\n",
    "    reader = csv.reader(f, delimiter=\",\")\n",
    "    for i in reader:\n",
    "      if i[1] == target_subset:\n",
    "        image_list.append(image_dir + i[0])\n",
    "print(image_list[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "detiYRZWEKvA"
   },
   "source": [
    "### Test out detections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LDezuFxm-xAf"
   },
   "source": [
    "Now we'll visualize some detections from our model to see how it performs. Each detection has a \"confidence score\" that describes the CNN's confidence that the detection is correct. Change the minimum confidence score (the first line of code) and re-run the code to check out how your \"confidence threshold\" affects the numbers of false positives and false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7I13IyhvEKvA"
   },
   "outputs": [],
   "source": [
    "min_score = 0.5 # this is the CNN's confidence that the detection is correct\n",
    "detection_iterations = 10 # max number of images to visualize\n",
    "\n",
    "visualize = True\n",
    "\n",
    "detections = {}\n",
    "\n",
    "total_time = 0\n",
    "\n",
    "count = 0\n",
    "shuffle(image_list)\n",
    "\n",
    "for image_path in image_list:\n",
    "    if count > detection_iterations:\n",
    "        break\n",
    "    else:\n",
    "        count +=1\n",
    "        \n",
    "    image = read_image_bgr(image_path)\n",
    "\n",
    "    if visualize:\n",
    "        # copy to draw on\n",
    "        draw = image.copy()\n",
    "        draw = cv2.cvtColor(draw, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # preprocess image for network\n",
    "    image = preprocess_image(image)\n",
    "    image, scale = resize_image(image)\n",
    "\n",
    "    # process image\n",
    "    start = time.time()\n",
    "    boxes, scores, labels = model.predict_on_batch(np.expand_dims(image, axis=0))\n",
    "    total_time += time.time() - start\n",
    "\n",
    "    # correct for image scale\n",
    "    boxes /= scale\n",
    "    if any(score >= min_score for score in scores[0]):\n",
    "        detections[image_path] = []\n",
    "\n",
    "    # visualize detections\n",
    "    for box, score, label in zip(boxes[0], scores[0], labels[0]):\n",
    "        # scores are sorted so we can break\n",
    "        if score < min_score:\n",
    "            break\n",
    "\n",
    "        #print(score)\n",
    "        #print(box)\n",
    "\n",
    "        # TODO this does create a slight error in the boxes, might be worth doing something like\n",
    "        # list(map(str, box) but then would need to cast on the other end back to float\n",
    "        b = box.astype(int)\n",
    "        detections[image_path].append({\"box\" : b, \"label\" : label, \"score\" : score})\n",
    "\n",
    "        if visualize:\n",
    "            color = label_color(label)\n",
    "\n",
    "            # b = box.astype(int)\n",
    "            draw_box(draw, b, color=color)\n",
    "\n",
    "            caption = \"{} {:.3f}\".format(labels_to_names[label], score)\n",
    "            draw_caption(draw, b, caption)\n",
    "\n",
    "    if any(score >= min_score for score in scores[0]):\n",
    "        if visualize:\n",
    "            plt.figure(figsize=(10, 10))\n",
    "            plt.axis('off')\n",
    "            plt.imshow(draw)\n",
    "            plt.show()\n",
    "    \n",
    "print(\"Finished, time per image:\", total_time/len(image_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P_Khk5BWEKvD"
   },
   "source": [
    "### Run Detections on all tiles\n",
    "This section repeats the process we just tested for all tiles that make up our orthomosaic. If you want to experiment, you can vary the confidence threshold and the amount of time the model trains, then look at how it affects the resulting detections.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Tvuim_vEKvE"
   },
   "outputs": [],
   "source": [
    "visualize = False\n",
    "min_score = 0.5 # this is the CNN's confidence that the detection is correct\n",
    "\n",
    "detections = {}\n",
    "\n",
    "total_time = 0\n",
    "\n",
    "for image_path in image_list:\n",
    "       \n",
    "    image = read_image_bgr(image_path)\n",
    "\n",
    "    if visualize:\n",
    "        # copy to draw on\n",
    "        draw = image.copy()\n",
    "        draw = cv2.cvtColor(draw, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # preprocess image for network\n",
    "    image = preprocess_image(image)\n",
    "    image, scale = resize_image(image)\n",
    "\n",
    "    # process image\n",
    "    start = time.time()\n",
    "    boxes, scores, labels = model.predict_on_batch(np.expand_dims(image, axis=0))\n",
    "    total_time += time.time() - start\n",
    "\n",
    "    # correct for image scale\n",
    "    boxes /= scale\n",
    "    if any(score >= min_score for score in scores[0]):\n",
    "        detections[image_path] = []\n",
    "\n",
    "    # visualize detections\n",
    "    for box, score, label in zip(boxes[0], scores[0], labels[0]):\n",
    "        # scores are sorted so we can break\n",
    "        if score < min_score:\n",
    "            break\n",
    "\n",
    "        #print(score)\n",
    "        #print(box)\n",
    "\n",
    "        # TODO this does create a slight error in the boxes, might be worth doing something like\n",
    "        # list(map(str, box) but then would need to cast on the other end back to float\n",
    "        b = box.astype(int)\n",
    "        detections[image_path].append({\"box\" : b, \"label\" : label, \"score\" : score})\n",
    "\n",
    "        if visualize:\n",
    "            color = label_color(label)\n",
    "\n",
    "            # b = box.astype(int)\n",
    "            draw_box(draw, b, color=color)\n",
    "\n",
    "            caption = \"{} {:.3f}\".format(labels_to_names[label], score)\n",
    "            draw_caption(draw, b, caption)\n",
    "\n",
    "    if any(score >= min_score for score in scores[0]):\n",
    "        if visualize:\n",
    "            plt.figure(figsize=(10, 10))\n",
    "            plt.axis('off')\n",
    "            plt.imshow(draw)\n",
    "            plt.show()\n",
    "    \n",
    "print(\"Finished, time per image:\", total_time/len(image_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4P3nBtjv81_J"
   },
   "source": [
    "Run an evaluation script to get the mean average precision (mAP) of the CNN. \n",
    "\n",
    "mAP is a model evaluation metric that is relative (aka it can be challenging to compare mAP values across datasets), but a great general metric for different models and approaches to detection objects on the same dataset. \n",
    "\n",
    "Read more about mAP here: https://tarangshah.com/blog/2018-01-27/what-is-map-understanding-the-statistic-of-choice-for-comparing-object-detection-models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5CKEvUTb1JC0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keras-retinanet' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "! keras-retinanet/keras_retinanet/bin/evaluate.py csv data/annotations_test.csv data/classes.csv snapshots/test_model.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-SyoUJ6REKvJ"
   },
   "source": [
    "### Export detections##\n",
    "Write out the detections to a json file that can be used in a GIS for  spatial databases and/or visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z-zIVPAVEKvH"
   },
   "outputs": [],
   "source": [
    "class MyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        else:\n",
    "            return super(MyEncoder, self).default(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qd3NXa3wEKvK"
   },
   "outputs": [],
   "source": [
    "with open('data/new_detections.json', 'w') as fp:\n",
    "    json.dump(detections, fp, cls=MyEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z-6WjaR1EKvM"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download(\"/content/data/new_detections.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export model (if wanted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that this is generally only necessary if we are planning to shop the model around to different datasets\n",
    "files.download(\"/content/snapshots/test_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### At the end of this script you should have a single JSON file downloaded. Drop this in the Google Drive folder so it can be exported in the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next steps:\n",
    "\n",
    "5) export CNN outputs"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "full_seal_detection_workflow.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
