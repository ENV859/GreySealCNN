{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/gl7176/GreySealCNN/blob/master/full_seal_detection_workflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "#####  <center> Be sure to update this hyperlink above if you clone and want to point to a different GitHub </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8XElcTShEQkh"
   },
   "source": [
    "# Set up the Model and Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AyeiBgQB1skx"
   },
   "source": [
    "Download all data from Google Drive onto the Co.Lab machine. This section sets up the environment of our virtual machine, then downloads tiles from our mosaic onto that virtual machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "le3bfsAjzSP_"
   },
   "outputs": [],
   "source": [
    "!pip install -U -q PyDrive\n",
    "import os\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "# 1. Authenticate and create the PyDrive client.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "\n",
    "# choose a local (colab) directory to store the data.\n",
    "local_download_path = os.path.expanduser('data')\n",
    "try:\n",
    "  os.makedirs(local_download_path)\n",
    "except: pass\n",
    "\n",
    "# 2. Auto-iterate using the query syntax\n",
    "#    https://developers.google.com/drive/v2/web/search-parameters\n",
    "#    The mess of characters before \"in parents\" is the address of the google drive folder getting pulled\n",
    "#    (when you open the folder in google drive on a browser, it is the mess of characters at the end of the URL)\n",
    "\n",
    "file_list = drive.ListFile(\n",
    "    {'q': \"'1INuRNVKvKMy8L_Nb6lmoVbyvScWK0-0D' in parents\"}).GetList()\n",
    "\n",
    "#    this bit pulls every file in the directory specified above\n",
    "count = 0\n",
    "for f in file_list:\n",
    "  count += 1\n",
    "  if count % 10 == 0:\n",
    "    print(count)\n",
    "  # 3. Create & download by id.\n",
    "  fname = os.path.join(local_download_path, f['title'])\n",
    "  f_ = drive.CreateFile({'id': f['id']})\n",
    "  f_.GetContentFile(fname)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LvOQTEJV8pHC"
   },
   "outputs": [],
   "source": [
    "! ls data | head -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N-VXXOfd-LXa"
   },
   "outputs": [],
   "source": [
    "#! pip uninstall --yes tensorflow\n",
    "#! pip uninstall --yes keras\n",
    "! pip install keras==2.4\n",
    "! pip install tensorflow==2.3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n-xdfna11L2F"
   },
   "source": [
    "### Install the Convolutional Neural Network that will do the detections. \n",
    "\n",
    "This section pulls code for a CNN called \"retinanet\", an existing model that is already trained for object detection, which we will further train for our task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_jb7YRXlEUUg"
   },
   "outputs": [],
   "source": [
    "! git clone https://github.com/fizyr/keras-retinanet/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RuULO44w6yx8"
   },
   "outputs": [],
   "source": [
    "% cd keras-retinanet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h7c-eGm41fjg"
   },
   "source": [
    "Install the \"retinanet\" code so we can run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pw8ZT-aU99ty"
   },
   "outputs": [],
   "source": [
    "! pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uu_IOPbC44SC"
   },
   "outputs": [],
   "source": [
    "! python setup.py build_ext --inplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WJZItqKH7GHS"
   },
   "outputs": [],
   "source": [
    "% cd ../"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P-XDi2Ct1nM8"
   },
   "source": [
    "Download the pre-trained model that we will use as a starting point for our seal detecting CNN. This includes weights and parameters that have been generated by past training of retinanet on a generic dataset of miscellaneous objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "33S3M0SoI1JI"
   },
   "outputs": [],
   "source": [
    "! wget -P data \"https://github.com/fizyr/keras-retinanet/releases/download/0.5.1/resnet50_coco_best_v2.1.0.h5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LYBUnE6wEKuq"
   },
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yzdoAR2GEKuv"
   },
   "source": [
    "#### Now we'll actually train the model. \n",
    "\n",
    "We're giving it the pre-trained weights that we downloaded above, and then we're telling it to use the training data in annotations.csv and to run for 10 epochs each with 20 steps with a batch-size of two. An epoch is a group of steps after which the model calculates its accuracy; a step is an increment of training the model on one batch or subset of files.\n",
    "\n",
    "This process could take 5-30 minutes, but you can stop it whenever you'd like as long as it has saved a couple models to the snapshots directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mcn229SrEKuw"
   },
   "outputs": [],
   "source": [
    "! keras-retinanet/keras_retinanet/bin/train.py --weights data/resnet50_coco_best_v2.1.0.h5 --epochs 10 --steps 20 --batch-size 2 csv data/annotations.csv data/classes.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iBRyzRLp4CyH"
   },
   "source": [
    "Let's take a look at the format of the annotations to see how it is reading them. The format here is:\n",
    "\n",
    "`filename, x1, y1, x2, y2`\n",
    "\n",
    "These X,Y pairs describe \\(1) the top-left and \\(2) the bottom-right corners of each box, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8D9p8G583243"
   },
   "outputs": [],
   "source": [
    "! head data/annotations.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "apHCQjSo8WC2"
   },
   "source": [
    "We are now done training the model, and we want to see how it performs on our data! This next section converts the model from training mode to inference mode so it can be used to detection seals. This conversion process take a little time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8YyMralKEKuz"
   },
   "outputs": [],
   "source": [
    "! keras-retinanet/keras_retinanet/bin/convert_model.py snapshots/resnet50_csv_10.h5 snapshots/test_model.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vu3yk48fEKu2"
   },
   "source": [
    "# Run Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GbTLLmbGEKu2"
   },
   "source": [
    "Now we wil get into the bulk of the python code: first we'll load the necessary python modules to handle and process our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bj-WmUXiEKu3"
   },
   "outputs": [],
   "source": [
    "# show images inline\n",
    "%matplotlib inline\n",
    "\n",
    "# automatically reload modules when they have changed\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import keras\n",
    "import keras\n",
    "\n",
    "# import keras_retinanet\n",
    "from keras_retinanet import models\n",
    "from keras_retinanet.utils.image import read_image_bgr, preprocess_image, resize_image\n",
    "from keras_retinanet.utils.visualization import draw_box, draw_caption\n",
    "from keras_retinanet.utils.colors import label_color\n",
    "\n",
    "# import miscellaneous modules\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "from random import shuffle\n",
    "\n",
    "# set tf backend to allow memory to grow, instead of claiming everything\n",
    "import tensorflow as tf\n",
    "\n",
    "def get_session():\n",
    "    #config = tf.ConfigProto()\n",
    "    # deprecated\n",
    "    config = tf.compat.v1.ConfigProto()    \n",
    "    config.gpu_options.allow_growth = True\n",
    "    #return tf.Session(config=config)\n",
    "    # deprecated\n",
    "    return tf.compat.v1.Session(config=config)\n",
    "\n",
    "# use this environment flag to change which GPU to use\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "# set the modified tf session as backend in keras\n",
    "#keras.backend.tensorflow_backend.set_session(get_session())\n",
    "# deprecated\n",
    "tf.compat.v1.keras.backend.set_session(get_session())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w987GUwIEKu5"
   },
   "source": [
    "## Load RetinaNet model\n",
    "\n",
    "Now we will load the model that you just converted into inference mode, it is called `test_model.h5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VDo883mYEKu6"
   },
   "outputs": [],
   "source": [
    "model_path = 'snapshots/test_model.h5'\n",
    "\n",
    "print(model_path)\n",
    "\n",
    "# load retinanet model\n",
    "model = models.load_model(model_path, backbone_name='resnet50')\n",
    "\n",
    "# you can check out a model summary by uncommenting this line\n",
    "#print(model.summary())\n",
    "\n",
    "# load label to names mapping for visualization purposes\n",
    "labels_to_names = {0: 'seal'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2jfMnDLVEKu9"
   },
   "source": [
    "## Load imagery\n",
    "\n",
    "Now we will load all of the images that we downloaded into our data directory during setup (the tiles from our orthomosaic). Just to check, we'll print out a count of those images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7QjtUN5wEKu9"
   },
   "outputs": [],
   "source": [
    "# load imagery\n",
    "image_dir = \"data/\"\n",
    "\n",
    "image_list = []\n",
    "for root, dirs, files in os.walk(image_dir):  \n",
    "    for filename in files:\n",
    "        if filename.lower().endswith(('.png')):\n",
    "            image_list.append(image_dir + filename)\n",
    "print(len(image_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "detiYRZWEKvA"
   },
   "source": [
    "## Test out detections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LDezuFxm-xAf"
   },
   "source": [
    "Now we'll visualize some detections from our model to see how it performs. Each detection has a \"confidence score\" that describes the CNN's confidence that the detection is correct. Change the minimum confidence score (the first line of code) and re-run the code to check out how your \"confidence threshold\" affects the numbers of false positives and false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7I13IyhvEKvA"
   },
   "outputs": [],
   "source": [
    "min_score = 0.5 # this is the CNN's confidence that the detection is correct\n",
    "detection_iterations = 10 # max number of images to visualize\n",
    "\n",
    "visualize = True\n",
    "\n",
    "detections = {}\n",
    "\n",
    "total_time = 0\n",
    "\n",
    "count = 0\n",
    "shuffle(image_list)\n",
    "\n",
    "# GDL added this, necessary for multiple category labels\n",
    "labels_to_names = {0: 'Adult', 1: 'Pup', 2: 'Unknown'}\n",
    "\n",
    "for image_path in image_list:\n",
    "    if count > detection_iterations:\n",
    "        break\n",
    "    else:\n",
    "        count +=1\n",
    "        \n",
    "    image = read_image_bgr(image_path)\n",
    "\n",
    "    if visualize:\n",
    "        # copy to draw on\n",
    "        draw = image.copy()\n",
    "        draw = cv2.cvtColor(draw, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # preprocess image for network\n",
    "    image = preprocess_image(image)\n",
    "    image, scale = resize_image(image)\n",
    "\n",
    "    # process image\n",
    "    start = time.time()\n",
    "    boxes, scores, labels = model.predict_on_batch(np.expand_dims(image, axis=0))\n",
    "    total_time += time.time() - start\n",
    "\n",
    "    # correct for image scale\n",
    "    boxes /= scale\n",
    "    if any(score >= min_score for score in scores[0]):\n",
    "        detections[image_path] = []\n",
    "\n",
    "    # visualize detections\n",
    "    for box, score, label in zip(boxes[0], scores[0], labels[0]):\n",
    "        # scores are sorted so we can break\n",
    "        if score < min_score:\n",
    "            break\n",
    "\n",
    "        #print(score)\n",
    "        #print(box)\n",
    "\n",
    "        # TODO this does create a slight error in the boxes, might be worth doing something like\n",
    "        # list(map(str, box) but then would need to cast on the other end back to float\n",
    "        b = box.astype(int)\n",
    "        detections[image_path].append({\"box\" : b, \"label\" : label, \"score\" : score})\n",
    "\n",
    "        if visualize:\n",
    "            color = label_color(label)\n",
    "\n",
    "            # b = box.astype(int)\n",
    "            draw_box(draw, b, color=color)\n",
    "\n",
    "            caption = \"{} {:.3f}\".format(labels_to_names[label], score)\n",
    "            draw_caption(draw, b, caption)\n",
    "\n",
    "    if any(score >= min_score for score in scores[0]):\n",
    "        if visualize:\n",
    "            plt.figure(figsize=(10, 10))\n",
    "            plt.axis('off')\n",
    "            plt.imshow(draw)\n",
    "            plt.show()\n",
    "    \n",
    "print(\"Finished, time per image:\", total_time/len(image_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P_Khk5BWEKvD"
   },
   "source": [
    "## Run Detections on all tiles ##\n",
    "This section repeats the process we just tested for all tiles that make up our orthomosaic. If you want to experiment, you can vary the confidence threshold and the amount of time the model trains, then look at how it affects the resulting detections.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Tvuim_vEKvE"
   },
   "outputs": [],
   "source": [
    "visualize = False\n",
    "min_score = 0.5 # this is the CNN's confidence that the detection is correct\n",
    "\n",
    "detections = {}\n",
    "\n",
    "total_time = 0\n",
    "\n",
    "for image_path in image_list:\n",
    "       \n",
    "    image = read_image_bgr(image_path)\n",
    "\n",
    "    if visualize:\n",
    "        # copy to draw on\n",
    "        draw = image.copy()\n",
    "        draw = cv2.cvtColor(draw, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # preprocess image for network\n",
    "    image = preprocess_image(image)\n",
    "    image, scale = resize_image(image)\n",
    "\n",
    "    # process image\n",
    "    start = time.time()\n",
    "    boxes, scores, labels = model.predict_on_batch(np.expand_dims(image, axis=0))\n",
    "    total_time += time.time() - start\n",
    "\n",
    "    # correct for image scale\n",
    "    boxes /= scale\n",
    "    if any(score >= min_score for score in scores[0]):\n",
    "        detections[image_path] = []\n",
    "\n",
    "    # visualize detections\n",
    "    for box, score, label in zip(boxes[0], scores[0], labels[0]):\n",
    "        # scores are sorted so we can break\n",
    "        if score < min_score:\n",
    "            break\n",
    "\n",
    "        #print(score)\n",
    "        #print(box)\n",
    "\n",
    "        # TODO this does create a slight error in the boxes, might be worth doing something like\n",
    "        # list(map(str, box) but then would need to cast on the other end back to float\n",
    "        b = box.astype(int)\n",
    "        detections[image_path].append({\"box\" : b, \"label\" : label, \"score\" : score})\n",
    "\n",
    "        if visualize:\n",
    "            color = label_color(label)\n",
    "\n",
    "            # b = box.astype(int)\n",
    "            draw_box(draw, b, color=color)\n",
    "\n",
    "            caption = \"{} {:.3f}\".format(labels_to_names[label], score)\n",
    "            draw_caption(draw, b, caption)\n",
    "\n",
    "    if any(score >= min_score for score in scores[0]):\n",
    "        if visualize:\n",
    "            plt.figure(figsize=(10, 10))\n",
    "            plt.axis('off')\n",
    "            plt.imshow(draw)\n",
    "            plt.show()\n",
    "    \n",
    "print(\"Finished, time per image:\", total_time/len(image_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4P3nBtjv81_J"
   },
   "source": [
    "Run an evaluation script to get the mean average precision (mAP) of the CNN. \n",
    "\n",
    "mAP is a model evaluation metric that is relative (aka it can be challenging to compare mAP values across datasets), but a great general metric for different models and approaches to detection objects on the same dataset. \n",
    "\n",
    "Read more about mAP here: https://tarangshah.com/blog/2018-01-27/what-is-map-understanding-the-statistic-of-choice-for-comparing-object-detection-models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5CKEvUTb1JC0"
   },
   "outputs": [],
   "source": [
    "! keras-retinanet/keras_retinanet/bin/evaluate.py csv data/annotations.csv data/classes.csv snapshots/test_model.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-SyoUJ6REKvJ"
   },
   "source": [
    "## Export detections##\n",
    "Write out the detections to a json file that can be used in a GIS for  spatial databases and/or visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z-zIVPAVEKvH"
   },
   "outputs": [],
   "source": [
    "class MyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        else:\n",
    "            return super(MyEncoder, self).default(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qd3NXa3wEKvK"
   },
   "outputs": [],
   "source": [
    "with open('data/new_detections.json', 'w') as fp:\n",
    "    json.dump(detections, fp, cls=MyEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z-6WjaR1EKvM"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "full_seal_detection_workflow.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
